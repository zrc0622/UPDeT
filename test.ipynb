{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class GumbelSoftmaxLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, temperature=1.0):\n",
    "        super(GumbelSoftmaxLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return self.gumbel_softmax(logits, self.temperature)\n",
    "    \n",
    "    def gumbel_softmax(self, logits, temperature):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()  # Sample from Gumbel(0,1)\n",
    "        gumbels = (logits + gumbels) / temperature\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "        return y_soft\n",
    "\n",
    "class GumbelSoftmaxNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, temperature=1.0):\n",
    "        super(GumbelSoftmaxNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gumbel_softmax_layer = GumbelSoftmaxLayer(hidden_dim, output_dim, temperature)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.gumbel_softmax_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "tensor([[[6.0728e-06, 1.6949e-06, 9.9993e-01, 6.6055e-06, 5.3866e-05,\n",
      "          6.7894e-06]],\n",
      "\n",
      "        [[4.0055e-05, 2.4352e-01, 7.4340e-01, 2.2581e-04, 2.5643e-07,\n",
      "          1.2815e-02]],\n",
      "\n",
      "        [[5.0849e-04, 1.2980e-03, 2.3850e-05, 3.6126e-03, 9.9448e-01,\n",
      "          7.6552e-05]],\n",
      "\n",
      "        [[9.9970e-01, 2.8638e-04, 3.6607e-07, 3.2177e-08, 5.9015e-08,\n",
      "          9.8429e-06]],\n",
      "\n",
      "        [[9.8945e-01, 7.2545e-03, 3.1459e-05, 5.4463e-05, 3.2127e-03,\n",
      "          8.7624e-07]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "next_t = GumbelSoftmaxNetwork(32, 64, 6, 0.15)\n",
    "\n",
    "random_tensor = torch.zeros(5, 1, 32)\n",
    "print(random_tensor)\n",
    "\n",
    "new_tensor = next_t(random_tensor)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = not a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "生成的向量是相互正交的。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gram_schmidt(vectors):\n",
    "    orthogonal_vectors = []\n",
    "    for v in vectors:\n",
    "        w = v.clone()\n",
    "        for u in orthogonal_vectors:\n",
    "            w -= (u @ w) * u\n",
    "        w /= torch.norm(w)\n",
    "        orthogonal_vectors.append(w)\n",
    "    return torch.stack(orthogonal_vectors)\n",
    "\n",
    "def generate_orthogonal_vectors(n, m):\n",
    "    # 生成n个维度为m的随机向量\n",
    "    random_vectors = torch.randn(n, m)\n",
    "    # 进行Gram-Schmidt正交化\n",
    "    orthogonal_vectors = gram_schmidt(random_vectors)\n",
    "    return orthogonal_vectors\n",
    "\n",
    "def check_orthogonality(vectors, tol=1e-6):\n",
    "    n = vectors.size(0)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dot_product = torch.dot(vectors[i], vectors[j])\n",
    "            if torch.abs(dot_product) > tol:\n",
    "                return False, i, j, dot_product\n",
    "    return True, None, None, None\n",
    "\n",
    "# 示例使用\n",
    "n = 3  # 生成3个向量\n",
    "m = 5  # 每个向量的维度为5\n",
    "orthogonal_vectors = generate_orthogonal_vectors(n, m)\n",
    "print(orthogonal_vectors.shape)\n",
    "# 检查正交性\n",
    "is_orthogonal, i, j, dot_product = check_orthogonality(orthogonal_vectors)\n",
    "if is_orthogonal:\n",
    "    print(\"生成的向量是相互正交的。\")\n",
    "else:\n",
    "    print(f\"向量 {i} 和 {j} 不是正交的，它们的点积为 {dot_product}。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
