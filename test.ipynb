{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7597)\n",
      "tensor(-0.1707)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def kl_loss_with_uniform_distribution(probabilities):\n",
    "    # probabilities shape: (36, 6), 每个step的6个动作的执行概率\n",
    "\n",
    "    # 理想的均匀分布，目标是每个动作的概率都是1/6\n",
    "    uniform_distribution = torch.full_like(probabilities, 1.0 / 6)\n",
    "\n",
    "    # 计算KL散度\n",
    "    kl_divergence = F.kl_div(probabilities.log(), uniform_distribution, reduction='batchmean')\n",
    "\n",
    "    return kl_divergence\n",
    "\n",
    "def kl_loss_with_uniform_distribution2(probabilities):\n",
    "    # probabilities shape: (36, 6), 每个step的6个动作的执行概率\n",
    "    probabilities = probabilities.mean(0)\n",
    "\n",
    "    # 理想的均匀分布，目标是每个动作的概率都是1/6\n",
    "    uniform_distribution = torch.full_like(probabilities, 1.0 / 6)\n",
    "\n",
    "    # 计算KL散度\n",
    "    kl_divergence = F.kl_div(probabilities.log(), uniform_distribution, reduction='batchmean')\n",
    "\n",
    "    return kl_divergence\n",
    "\n",
    "probs = torch.rand((36, 6))\n",
    "loss = kl_loss_with_uniform_distribution(probs)\n",
    "loss2 = kl_loss_with_uniform_distribution2(probs)\n",
    "print(loss)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class GumbelSoftmaxLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, temperature=1.0):\n",
    "        super(GumbelSoftmaxLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return self.gumbel_softmax(logits, self.temperature)\n",
    "    \n",
    "    def gumbel_softmax(self, logits, temperature):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()  # Sample from Gumbel(0,1)\n",
    "        gumbels = (logits + gumbels) / temperature\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "        return y_soft\n",
    "\n",
    "class GumbelSoftmaxNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, temperature=1.0):\n",
    "        super(GumbelSoftmaxNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gumbel_softmax_layer = GumbelSoftmaxLayer(hidden_dim, output_dim, temperature)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.gumbel_softmax_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5346, 0.3477, 0.6577, 0.7236, 0.6576, 0.5701, 0.0972, 0.9726,\n",
      "          0.5077, 0.7952, 0.5268, 0.2911, 0.3786, 0.0643, 0.3246, 0.7095,\n",
      "          0.7140, 0.5855, 0.9744, 0.8222, 0.2649, 0.0059, 0.8495, 0.5385,\n",
      "          0.7412, 0.9559, 0.2478, 0.5536, 0.0400, 0.5015, 0.1610, 0.7644]],\n",
      "\n",
      "        [[0.0535, 0.7681, 0.9845, 0.0975, 0.0649, 0.4816, 0.8915, 0.6737,\n",
      "          0.3466, 0.5498, 0.0342, 0.6114, 0.4362, 0.1277, 0.7215, 0.3138,\n",
      "          0.9904, 0.9386, 0.4814, 0.2683, 0.9539, 0.5305, 0.8033, 0.2631,\n",
      "          0.6895, 0.8195, 0.0763, 0.1464, 0.7540, 0.5326, 0.3904, 0.9345]],\n",
      "\n",
      "        [[0.8986, 0.4886, 0.9672, 0.2825, 0.2132, 0.4133, 0.0741, 0.8025,\n",
      "          0.1921, 0.8384, 0.9725, 0.7110, 0.3050, 0.3421, 0.6498, 0.5822,\n",
      "          0.4542, 0.9894, 0.4632, 0.0188, 0.2366, 0.4250, 0.7348, 0.0617,\n",
      "          0.8287, 0.9939, 0.7216, 0.4708, 0.1142, 0.1647, 0.7365, 0.8480]],\n",
      "\n",
      "        [[0.8470, 0.5153, 0.5823, 0.0641, 0.6579, 0.9118, 0.0278, 0.4377,\n",
      "          0.6372, 0.2990, 0.4436, 0.1083, 0.9653, 0.4182, 0.8367, 0.6208,\n",
      "          0.2133, 0.0980, 0.7875, 0.7762, 0.3605, 0.5274, 0.7063, 0.5914,\n",
      "          0.8377, 0.7502, 0.6609, 0.2618, 0.2072, 0.8847, 0.1592, 0.1329]],\n",
      "\n",
      "        [[0.9784, 0.3769, 0.1663, 0.6207, 0.8637, 0.6843, 0.1502, 0.5273,\n",
      "          0.6035, 0.7202, 0.5795, 0.7647, 0.6127, 0.8621, 0.8760, 0.8753,\n",
      "          0.6813, 0.7772, 0.4929, 0.1843, 0.5617, 0.7548, 0.6321, 0.4686,\n",
      "          0.6125, 0.7655, 0.7866, 0.0973, 0.2106, 0.3105, 0.1150, 0.1639]]])\n",
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "next_t = GumbelSoftmaxNetwork(32, 64, 6, 0.5)\n",
    "\n",
    "random_tensor = torch.rand(5, 1, 32)\n",
    "print(random_tensor)\n",
    "\n",
    "new_tensor = next_t(random_tensor)\n",
    "print(new_tensor.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "b = not a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "生成的向量是相互正交的。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gram_schmidt(vectors):\n",
    "    orthogonal_vectors = []\n",
    "    for v in vectors:\n",
    "        w = v.clone()\n",
    "        for u in orthogonal_vectors:\n",
    "            w -= (u @ w) * u\n",
    "        w /= torch.norm(w)\n",
    "        orthogonal_vectors.append(w)\n",
    "    return torch.stack(orthogonal_vectors)\n",
    "\n",
    "def generate_orthogonal_vectors(n, m):\n",
    "    # 生成n个维度为m的随机向量\n",
    "    random_vectors = torch.randn(n, m)\n",
    "    # 进行Gram-Schmidt正交化\n",
    "    orthogonal_vectors = gram_schmidt(random_vectors)\n",
    "    return orthogonal_vectors\n",
    "\n",
    "def check_orthogonality(vectors, tol=1e-6):\n",
    "    n = vectors.size(0)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dot_product = torch.dot(vectors[i], vectors[j])\n",
    "            if torch.abs(dot_product) > tol:\n",
    "                return False, i, j, dot_product\n",
    "    return True, None, None, None\n",
    "\n",
    "# 示例使用\n",
    "n = 3  # 生成3个向量\n",
    "m = 5  # 每个向量的维度为5\n",
    "orthogonal_vectors = generate_orthogonal_vectors(n, m)\n",
    "print(orthogonal_vectors.shape)\n",
    "# 检查正交性\n",
    "is_orthogonal, i, j, dot_product = check_orthogonality(orthogonal_vectors)\n",
    "if is_orthogonal:\n",
    "    print(\"生成的向量是相互正交的。\")\n",
    "else:\n",
    "    print(f\"向量 {i} 和 {j} 不是正交的，它们的点积为 {dot_product}。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
